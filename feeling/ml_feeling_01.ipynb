{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle as pick\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from unidecode import unidecode\n",
    "from string import punctuation\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('rslp')\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "# stopwords = nltk.corpus.stopwords.words('portuguese') #remove palavras desnecessarias\n",
    "\n",
    "# nltk.download('rslp')\n",
    "# stemmer = nltk.stem.RSLPStemmer()#deixa somente a raiz da palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('..', 'storage', 'base_sentiment.1.0.2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['classificacao'] = df.sentiment.replace([\"negative\", 'positive'], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in df['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_whitespace = nltk.tokenize.WhitespaceTokenizer()\n",
    "token_frase = token_whitespace.tokenize(all_words) \n",
    "freq = nltk.FreqDist(token_frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq = pd.DataFrame({'palavra': list(freq.keys()), 'frequencia': list(freq.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq.nlargest(columns='frequencia', n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_irrelevantes = nltk.corpus.stopwords.words('portuguese')\n",
    "frase_processada = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for opiniao in df.text:\n",
    "    nova_frase = list()\n",
    "    palavra_texto = token_whitespace.tokenize(opiniao)\n",
    "\n",
    "    for palavra in palavra_texto:\n",
    "        if palavra not in palavras_irrelevantes:\n",
    "            nova_frase.append(palavra)\n",
    "    \n",
    "    frase_processada.append(' '.join(nova_frase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tratamento_1'] = frase_processada\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pontuacao = list()\n",
    "frase_processada = list()\n",
    "\n",
    "for ponto in punctuation:\n",
    "    pontuacao.append(ponto)\n",
    "\n",
    "palavras_irrelevantes = nltk.corpus.stopwords.words('portuguese')\n",
    "pontuacao_stopwords = pontuacao + palavras_irrelevantes\n",
    "\n",
    "for opiniao in df['tratamento_1']:\n",
    "    nova_frase = list()\n",
    "    palavra_texto = token_whitespace.tokenize(opiniao)\n",
    "\n",
    "    for palavra in palavra_texto:\n",
    "        if palavra not in pontuacao_stopwords:\n",
    "            nova_frase.append(palavra)\n",
    "\n",
    "    frase_processada.append(' '.join(nova_frase))\n",
    "\n",
    "df['tratamento_2'] = frase_processada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_acentos = [unidecode(texto) for texto in df['tratamento_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tratamento_3'] = sem_acentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_sem_acentos = [unidecode(texto) for texto in pontuacao_stopwords]\n",
    "frase_processada = list()\n",
    "\n",
    "for opiniao in df['tratamento_3']:\n",
    "    nova_frase = list()\n",
    "    palavra_texto = token_whitespace.tokenize(opiniao)\n",
    "\n",
    "    for palavra in palavra_texto:\n",
    "        if palavra not in stopwords_sem_acentos:\n",
    "            nova_frase.append(palavra)\n",
    "\n",
    "    frase_processada.append(' '.join(nova_frase))\n",
    "\n",
    "df['tratamento_4'] = frase_processada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_processada = list()\n",
    "for opiniao in df['tratamento_4']:\n",
    "    nova_frase = list()\n",
    "    opiniao = opiniao.lower()\n",
    "    palavra_texto = token_whitespace.tokenize(opiniao)\n",
    "\n",
    "    for palavra in palavra_texto:\n",
    "        if palavra not in stopwords_sem_acentos:\n",
    "            nova_frase.append(palavra)\n",
    "\n",
    "    frase_processada.append(' '.join(nova_frase))\n",
    "\n",
    "df['tratamento_5'] = frase_processada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.RSLPStemmer()\n",
    "\n",
    "frase_processada = list()\n",
    "for opiniao in df['tratamento_5']:\n",
    "    nova_frase = list()\n",
    "    palavra_texto = token_whitespace.tokenize(opiniao)\n",
    "\n",
    "    for palavra in palavra_texto:\n",
    "        nova_frase.append(stemmer.stem(palavra))\n",
    "\n",
    "    frase_processada.append(' '.join(nova_frase))\n",
    "\n",
    "df['tratamento_6'] = frase_processada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tread = df[['classificacao', 'tratamento_6']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pos = df_tread.query('classificacao == 1').values\n",
    "list_neg = df_tread.query('classificacao == 0').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_for_model(token_list):\n",
    "    for tokens in token_list:\n",
    "        yield dict([token, True] for token in tokens)\n",
    "\n",
    "positive_tokens_for_model = get_for_model(list_pos)\n",
    "negative_tokens_for_model = get_for_model(list_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_dataset = [(tweet_dict, \"Positive\") for tweet_dict in positive_tokens_for_model]\n",
    "negative_dataset = [(tweet_dict, \"Negative\") for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "training_data = dataset[:int((len(dataset)) / 100 * 80)]\n",
    "test_data = dataset[int((len(dataset)) / 100 * 20):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_data)\n",
    "\n",
    "print(\"Accuracy is:\", nltk.classify.accuracy(classifier, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tweet = \"oi! tudo bem? sim tudo Ã³timo\"\n",
    "\n",
    "custom_tokens = token_whitespace.tokenize(custom_tweet)\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37d749834043cc8bcf9500ddac590bcfc07020a4e948b1c7aab57c3f0cf1e0ef"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
