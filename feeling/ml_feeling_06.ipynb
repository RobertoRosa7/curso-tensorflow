{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from unidecode import unidecode\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\X308891\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('..', 'storage', 'base_sentiment.1.0.2.csv'))\n",
    "input = open(os.path.join('..', 'storage', 'mac_morpho.1.0.0.pkl'), 'rb')\n",
    "tagger = pickle.load(input)\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "token_whitespace = nltk.tokenize.WhitespaceTokenizer()\n",
    "stemmer = nltk.RSLPStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dict(text=df['text'].values, sentiment=df['sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 160\n",
    "texts = {'text': [], 'sentiment': []}\n",
    "\n",
    "\n",
    "def complete_text(text: str) -> str:\n",
    "    text = text + ' '\n",
    "    left = max_length - len(text)\n",
    "    \n",
    "    for _ in range(left):\n",
    "        text = text + '#'\n",
    "    \n",
    "    return text\n",
    "\n",
    "for i, word in enumerate(test['text']):\n",
    "    for index in range(0, len(word), max_length):\n",
    "        if len(word[index:]) < max_length:\n",
    "            texts['text'].append(complete_text(word[index:]))\n",
    "            texts['sentiment'].append(test.get('sentiment')[i])\n",
    "        else: \n",
    "            texts['text'].append(word[index:(index + max_length)])\n",
    "            texts['sentiment'].append(test.get('sentiment')[i])\n",
    "\n",
    "df_texts = pd.DataFrame(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mais uma vez, o Sr. Costner arrumou um filme p...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s, eu simplesmente não me importei com nenhum ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>epois esquecido até muito mais tarde, quando e...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cher. O problema é que ele sai como um garoto ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u único obstáculo parece estar vencendo Costne...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  Mais uma vez, o Sr. Costner arrumou um filme p...  negative\n",
       "1  s, eu simplesmente não me importei com nenhum ...  negative\n",
       "2  epois esquecido até muito mais tarde, quando e...  negative\n",
       "3  cher. O problema é que ele sai como um garoto ...  negative\n",
       "4  u único obstáculo parece estar vencendo Costne...  negative"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step one\n",
    "def step1(text):\n",
    "    return unidecode(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2 = lambda token: re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "# step3 = lambda token: re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step2(text):\n",
    "    return re.sub(r'\\w*\\d\\w*', '', text)  # remover caractres especiais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step3(text):\n",
    "    return re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)  # remover pontuações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step4(text):\n",
    "    return text.replace('  ', ' ')  # remover espaço duplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step5(text):\n",
    "    words = []\n",
    "\n",
    "    for sentence in tagger.tag(token_whitespace.tokenize(text)):\n",
    "        word, tag = sentence\n",
    "        if tag.startswith(\"N\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('V'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemm = lemmatizer.lemmatize(word, pos)\n",
    "        words.append(lemm)\n",
    "            \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step6(text):\n",
    "    return \" \".join(text for text in text.split() if text not in stopwords and len(text) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step7(text):\n",
    "    return stemmer.stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts['text_clean'] = df_texts['text'].map(step1).map(step2).map(step3).map(step4).map(step5).map(step6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         vez sr costner arrumou filme tempo necessario ...\n",
       "1         s simplesmente nao importei nenhum personagens...\n",
       "2         epois esquecido ate tarde nao importava person...\n",
       "3         cher problema sai garoto pensa melhor qualquer...\n",
       "4         u unico obstaculo parece vencendo costner fina...\n",
       "                                ...                        \n",
       "420887    voce pode adivinhar resto adaptacao romance pa...\n",
       "420888    s bagunca ainda agradavel fantasia colarinho a...\n",
       "420889    apenas ferramenta enredo historia amor segue n...\n",
       "420890    ncolores derivacoes reduzidas nivel interessan...\n",
       "420891    am final feliz foto nao realmente satisfatoria...\n",
       "Name: text_clean, Length: 420892, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_texts['text_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = FreqDist(token_whitespace.tokenize(' '.join([text for text in df_texts['text_clean']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('filme', 150476),\n",
       " ('nao', 145599),\n",
       " ('voce', 56709),\n",
       " ('sao', 35007),\n",
       " ('filmes', 31634)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq.most_common(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = df_texts.sample(frac=0.8, random_state=100)\n",
    "# test_df = df_texts[~df_texts.index.isin(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for w in df_texts.query('sentiment == \"positive\"')['text_clean'].values:\n",
    "        positive_cleaned_tokens_list.append(token_whitespace.tokenize(w))\n",
    "\n",
    "for w in df_texts.query('sentiment == \"negative\"')['text_clean'].values:\n",
    "        negative_cleaned_tokens_list.append(token_whitespace.tokenize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_dataset = [(tweet_dict, \"Positive\") for tweet_dict in document_features(positive_cleaned_tokens_list)]\n",
    "negative_dataset = [(tweet_dict, \"Negative\") for tweet_dict in document_features(negative_cleaned_tokens_list)]\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = np.split(dataset, [int(len(dataset)*0.8), int(len(dataset)*0.2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.7873744483448861\n",
      "Most Informative Features\n",
      "              desperdice = True           Negati : Positi =     51.7 : 1.0\n",
      "                    boll = True           Negati : Positi =     43.5 : 1.0\n",
      "              redentores = True           Negati : Positi =     33.9 : 1.0\n",
      "                   fedor = True           Negati : Positi =     30.0 : 1.0\n",
      "                     nan = True           Positi : Negati =     25.5 : 1.0\n",
      "                   anand = True           Positi : Negati =     24.9 : 1.0\n",
      "               crossfire = True           Positi : Negati =     24.9 : 1.0\n",
      "                   secar = True           Negati : Positi =     23.9 : 1.0\n",
      "                  gotham = True           Positi : Negati =     23.5 : 1.0\n",
      "                   lindy = True           Positi : Negati =     22.9 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Accuracy is: 0.7879832736387558 => sem stemmer\n",
    "# Accuracy is: 0.7873744483448861 => sem stemmer\n",
    "# Accuracy is: 0.7872586230450769 => com stemmer\n",
    "# Accuracy is: 0.7874219664166028 => com stemmer on position 4\n",
    "# Accuracy is: 0.7869289664225426 => com stemmer on position 2\n",
    "# Accuracy is: 0.7867062254613708 => com stemmer on position 0\n",
    "classifier = NaiveBayesClassifier.train(train)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a vida nem sempre e facil mas muitas vezes cabe a nos mudar nossa perspectiva e enxerga la de forma mais positiva nao importa o que voce esta passando agora contanto que nunca perca a esperanca na coisas boas que estao por vir'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = pd.Series(['A vida nem sempre é fácil, mas muitas vezes cabe a nós mudar nossa perspectiva e enxergá-la de forma mais positiva. Não importa o que você está passando agora, contanto que nunca perca a esperança nas coisas boas que estão por vir ']).map(step1).map(step2).map(step3).map(step4).map(step5)\n",
    "tt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            a\n",
       "1          vid\n",
       "2          nem\n",
       "3        sempr\n",
       "4            e\n",
       "5        facil\n",
       "6          mas\n",
       "7         muit\n",
       "8          vez\n",
       "9          cab\n",
       "10           a\n",
       "11          no\n",
       "12         mud\n",
       "13        noss\n",
       "14     perspec\n",
       "15           e\n",
       "16      enxerg\n",
       "17          la\n",
       "18          de\n",
       "19        form\n",
       "20        mais\n",
       "21        posi\n",
       "22         nao\n",
       "23      import\n",
       "24           o\n",
       "25         que\n",
       "26         voc\n",
       "27         est\n",
       "28        pass\n",
       "29        agor\n",
       "30     contant\n",
       "31         que\n",
       "32        nunc\n",
       "33        perc\n",
       "34           a\n",
       "35    esperanc\n",
       "36          na\n",
       "37        cois\n",
       "38         boa\n",
       "39         que\n",
       "40        esta\n",
       "41         por\n",
       "42         vir\n",
       "dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(token_whitespace.tokenize(tt[0])).map(step7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_frase = [\n",
    "    'Ontem eu não fui ao dentista.'\n",
    "    'Meu pai nunca gostou daquela cantora.',\n",
    "    'O vidro da janela nem está muito sujo.',\n",
    "    'Eu jamais pedirei sua ajuda.',\n",
    "    'Eu tampouco fui selecionada no processo de seleção da empresa.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: Positive => Ontem eu não fui ao dentista.Meu pai nunca gostou daquela cantora.\n",
      "predict: Positive => O vidro da janela nem está muito sujo.\n",
      "predict: Positive => Eu jamais pedirei sua ajuda.\n",
      "predict: Positive => Eu tampouco fui selecionada no processo de seleção da empresa.\n"
     ]
    }
   ],
   "source": [
    "for text in negative_frase:\n",
    "    predict = classifier.classify(dict([token, True] for token in [text]))\n",
    "    # predict = sia.polarity_scores(\"Ontem eu não fui ao dentista.\")\n",
    "\n",
    "    print(f'predict: {predict} => {text}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37d749834043cc8bcf9500ddac590bcfc07020a4e948b1c7aab57c3f0cf1e0ef"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
