{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone project 2 - SkimLit\n",
    "\n",
    "## SkimLit sequece problem: Many to one\n",
    "\n",
    "To investigate the efficacy of @week of daily low-dose oral prednisolone in improving pain, mobility, systemic low-grade inflamation in the short term and wether the effect would be sustained at @week in order adults with moderate to sever knee osteoarthritis (OA).\n",
    "\n",
    "## What we're going to cover\n",
    "\n",
    "* Download a text dataset (PubMed 200k RCT)\n",
    "* Writing a preprocessing function for our text data\n",
    "* Setting up multiple modelling experiments with differents levels of embeddings\n",
    "* Building a multimodal model to take in different source of data\n",
    "    - Replicating the model powering https://arxiv.org/abs/1710.06071\n",
    "* Finding the most wrong prediction example\n",
    "\n",
    "see: https://www.nltk.org/install.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset\n",
    "\n",
    "Since we'll be replicating a paper above (PubMed 200k RCT), let's download the data they used.\n",
    "\n",
    "We can do so from the authors GitHub: https://github.com/Franck-Dernoncourt/pubmed-rct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 1.3.5\n",
      "tensorflow: 2.9.3\n",
      "sklearn: 1.0.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import multiprocessing\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# add path root project to read helper fuctions\n",
    "sys.path.append(os.path.join('../'))\n",
    "\n",
    "from helper_functions import calculate_results\n",
    "\n",
    "print(f'pandas: {pd.__version__}')\n",
    "print(f'tensorflow: {tf.__version__}')\n",
    "print(f'sklearn: {skl.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_lists = ['tokenizers/punkt', 'stemmers/rslp', 'corpora/stopwords']\n",
    "\n",
    "for name in nltk_lists:\n",
    "    try:\n",
    "        nltk.data.find(name)\n",
    "    except LookupError:\n",
    "        nltk.download(name.split('/')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE = os.path.join('../../', 'storage')\n",
    "MODEL_PATH = f'{STORAGE}/models'\n",
    "NLP = f'{STORAGE}/nlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git\n",
    "# shutil.move('pubmed-rct', f'{NLP}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list dir and see your content\n",
    "os.listdir(f'{NLP}/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start our experiments using 20k dataset with number replaced by '@' sign\n",
    "data_dir = f'{NLP}/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all of the filenames in the target directory\n",
    "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "Now we've got some text data, it's time to become one with it.\n",
    "\n",
    "And one of the best ways to become one with the data is to...\n",
    "\n",
    "> Visualize, Visualize, Visualize\n",
    "\n",
    "So with that in mind, let's write a function to read in all of the lines of a target text file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to read the lines of a document\n",
    "def get_lines(filename):\n",
    "    \"\"\" \n",
    "    Reads filename (a txt filename) and returns the lines of a text as a list\n",
    "    Args:\n",
    "        filename:  a string  containing the target filepath\n",
    "    Returns:\n",
    "        A list of strings with one string per line from the target filename\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as file:\n",
    "        return file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in the training lines\n",
    "\n",
    "train_lines = get_lines(data_dir + 'train.txt') # read the lines with the training file\n",
    "train_lines[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about how we want our data to look\n",
    "\n",
    "How I think our data would be best represented\n",
    "\n",
    "```json\n",
    "[   \n",
    "    {\n",
    "        \"line_number\":0,\n",
    "        \"target\": \"BACKGROUND\",\n",
    "        \"text\":\" 'Serum levels of interleukin @ ( IL-@ ) , IL-@  and high-sensitivity C-reactive protein ( hsCRP ) were measured .\\n'\",\n",
    "        \"total_lines\": 11\n",
    "    }\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "Let's write a function which turns each of our dataset into the above format so we can continue to prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_with_line_numbers(filename):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        filename: (str) required\n",
    "    Returns:\n",
    "        A list of dictionaries of abstract line data.\n",
    "\n",
    "        Takes in filename, reads it contents and sorts through each line, extracting\n",
    "        things like the target label, the text of the sentence, how many sentences are\n",
    "        in the current abstract and what sentence number the target line is.\n",
    "    \"\"\"\n",
    "    input_lines = get_lines(filename) # get all lines from filename\n",
    "    abstract_lines = \"\" # create an empty abstract\n",
    "    abstract_sample = [] # create an empty list\n",
    "\n",
    "    # loop through each line in the target file\n",
    "    for line in input_lines:\n",
    "        if line.startswith('###'): # check if line start with ###\n",
    "            # get id from line\n",
    "            abstract_id = line\n",
    "\n",
    "            # reset the abstract string if the line is an ID line\n",
    "            abstract_lines = \"\" \n",
    "        elif line.isspace(): # check line is a new line\n",
    "            # split abstract into separate lines\n",
    "            abstract_line_split = abstract_lines.splitlines()\n",
    "\n",
    "            # iterate through each line in a single abstract and count them at the same time\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "                # create an empty dictionary for each line\n",
    "                line_data = {}\n",
    "                \n",
    "                # split target label from text\n",
    "                target_text_split = abstract_line.split('\\t')\n",
    "                \n",
    "                # get target label\n",
    "                line_data['target'] = target_text_split[0]\n",
    "                \n",
    "                # get target text and lower it\n",
    "                line_data['text'] = target_text_split[1].lower()\n",
    "                \n",
    "                # what number does the line appear in\n",
    "                line_data['number'] = abstract_line_number\n",
    "                \n",
    "                # how many total lines are there in the target abstract? (start from 0)\n",
    "                line_data['total_line'] = len(abstract_line_split) - 1\n",
    "                \n",
    "                # add line data to abstract sample list\n",
    "                abstract_sample.append(line_data)\n",
    "        else: # if the above conditions aren't fulfilled the line contains a labelled sentences \n",
    "            abstract_lines += line\n",
    "            \n",
    "    return abstract_sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from file and preprocess it\n",
    "\n",
    "train_samples = preprocess_text_with_line_numbers(data_dir + 'train.txt')\n",
    "val_samples = preprocess_text_with_line_numbers(data_dir + 'dev.txt') # dev is another name for validation data\n",
    "test_samples = preprocess_text_with_line_numbers(data_dir + 'test.txt')\n",
    "\n",
    "print(len(train_samples), len(val_samples), len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first abstract of our training data\n",
    "train_samples[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pandas to visualize our train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how are distribution of labels in training data\n",
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the length of different lines\n",
    "train_df['total_line'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert abstract text line to list\n",
    "train_sentences = train_df['text'].tolist()\n",
    "val_sentences = val_df['text'].tolist()\n",
    "test_sentences = test_df['text'].tolist()\n",
    "\n",
    "len(train_sentences), len(val_sentences), len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make numeric label (ML models require numeric label)\n",
    "\n",
    "see: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "\n",
    "see: https://en.wikipedia.org/wiki/Sparse_matrix\n",
    "\n",
    "**WARNING**: Tensorflow is uncompatible with matrix sparse so use hyperameter false in OneHotEncoder of sklearn preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoder\n",
    "one_hot_encoder = skl.preprocessing.OneHotEncoder(sparse=False) # we want non-sparse matrix\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df['target'].to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_df['target'].to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_df['target'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# check what trainining one hot encoder look like\n",
    "train_labels_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels ('target' column) and encode them into integers\n",
    "label_encoder = skl.preprocessing.LabelEncoder()\n",
    "train_labels_label_encoded = label_encoder.fit_transform(train_df['target'].to_numpy())\n",
    "val_labels_label_encoded = label_encoder.transform(val_df['target'].to_numpy())\n",
    "test_labels_label_encoded = label_encoder.transform(test_df['target'].to_numpy())\n",
    "\n",
    "# check what training label look like\n",
    "train_labels_label_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get class name and number of classes from LabelEncoder intances\n",
    "num_classes = len(label_encoder.classes_)\n",
    "class_names =  label_encoder.classes_\n",
    "\n",
    "num_classes, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0: Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model 0 baseline\n",
    "model_0 = Pipeline([ \n",
    "    ('tf-idf', TfidfVectorizer()),  # convert words to number using tf-idf\n",
    "    ('clf', MultinomialNB()) # model text\n",
    "])\n",
    "\n",
    "model_0_history = model_0.fit(train_sentences, train_labels_label_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.score(val_sentences, val_labels_label_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some predictions using our baseline\n",
    "model_0_preds = model_0.predict(val_sentences)\n",
    "model_0_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0_results = calculate_results(y_true=val_labels_label_encoded, y_pred=model_0_preds)\n",
    "model_results = pd.DataFrame({'model_0_results': model_0_results})\n",
    "model_results.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our data (the text) for deep sequence models\n",
    "\n",
    "Before we start building deeper models, we've got to create vectorization and embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many words there are in our train_sentences: https://arxiv.org/pdf/1710.06071.pdf\n",
    "sent_lens = [len(sentence.split()) for sentence in train_sentences]\n",
    "len(sent_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how long is each sentence on average?\n",
    "np.mean(sent_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how long is each sentence on average?\n",
    "sum(sent_lens)/len(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the distribuition look like?\n",
    "ax = plt.hist(sent_lens, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long of a sentence length covers 95% of examples?\n",
    "output_seq_len = int(np.percentile(sent_lens, 95))\n",
    "output_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length in the training set\n",
    "max(sent_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 68000\n",
    "\n",
    "# create text vectorizer\n",
    "text_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=max_tokens, # number of words in vocab\n",
    "                                                                               output_sequence_length=output_seq_len,\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt text vecotorizer to training sentences\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentence = random.choice(train_sentences)\n",
    "print(f'Text:\\n{target_sentence}')\n",
    "print(f'\\nLength of text: {len(target_sentence.split())}')\n",
    "print(f'\\nVectorizer text: {text_vectorizer([target_sentence])}')\n",
    "print(f'\\nLength Vectorizer: {len(text_vectorizer([target_sentence])[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many words in our training vocabulary\n",
    "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
    "\n",
    "print(f'Number of words in vocab: {len(rct_20k_text_vocab)}')\n",
    "print(f'The most common words: {rct_20k_text_vocab[:5]}')\n",
    "print(f'The least common words: {rct_20k_text_vocab[-5:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the config of text_vectorizer\n",
    "text_vectorizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create token embedding layer\n",
    "token_embed = tf.keras.layers.Embedding(input_dim=len(rct_20k_text_vocab), # length of vocabulary\n",
    "                                        output_dim=128, # NOTE: different embedding sizes result in \n",
    "                                        mask_zero=False, # use masking to handle variable sequence length\n",
    "                                        name='token_embedding'\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Sentence before vectorization:\\n {target_sentence}\\n')\n",
    "vectorized_sentence = text_vectorizer([target_sentence])\n",
    "print(f'Sentence after vectorization (before embedding):\\n{vectorized_sentence}\\n')\n",
    "embedded_sentence = token_embed(vectorized_sentence)\n",
    "print(f'Sentence after embedding:\\n{embedded_sentence}\\n')\n",
    "print(f'Sentence after embedding shape:\\n{embedded_sentence.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn our data into Tensorflow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the TensorSliceDataset's and turn them into prefected dataset\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE),\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Conv1D with token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Conv 1D model to process sequences\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs) # vectorize text input\n",
    "x = token_embed(x) # create embedding\n",
    "x = tf.keras.layers.Conv1D(filters=64, \n",
    "                           kernel_size=5, \n",
    "                           padding='same', \n",
    "                           activation='relu')(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x) # condense the output of our feature vector from conv layer\n",
    "outputs = tf.keras.layers.Dense(num_classes, \n",
    "                                activation='softmax')(x) # we have more than two class\n",
    "model_1 = tf.keras.Model(inputs, \n",
    "                         outputs,\n",
    "                         name='model_1_conv1d_token_embedding')\n",
    "\n",
    "# compile mode 1\n",
    "model_1.compile(loss=tf.keras.losses.CategoricalCrossentropy(), \n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "# show summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model with 10% of train_dataset\n",
    "model_1_history = model_1.fit(train_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_dataset)), # use only 10%\n",
    "                              epochs=3,\n",
    "                              validation_data=val_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_dataset)) # only validate on 10% of batches\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "model_1.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some predictions (out model predictions probabilities for each class)\n",
    "model_1_pred_prob = model_1.predict(val_dataset)\n",
    "model_1_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pred prob to classes\n",
    "model_1_preds = tf.argmax(model_1_pred_prob, axis=1)\n",
    "model_1_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_results = calculate_results(y_true=val_labels_label_encoded, y_pred=model_1_preds)\n",
    "\n",
    "model_results = pd.DataFrame({'model_0_results': model_0_results,\n",
    "                              'model_1_results': model_1_results})\n",
    "model_results.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Feature Extraction with Pretrained Model\n",
    "\n",
    "see: https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "\n",
    "see: http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc\n",
    "\n",
    "see: https://www.kaggle.com/code/chewzy/tutorial-how-to-train-your-custom-word-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_use = tf.keras.models.load_model(f'{MODEL_PATH}/tfhub_universal_sentence_encoder')\n",
    "\n",
    "tfhub_url = 'https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2'\n",
    "\n",
    "# create a Keras Layer using the pretrained layer from tensorflow hub\n",
    "sentence_encoder_layer = hub.KerasLayer(tfhub_url, \n",
    "                                        input_shape=[], \n",
    "                                        dtype=tf.string,\n",
    "                                        trainable=False, \n",
    "                                        name='USE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the pretrained embedding on a random sentences\n",
    "random_train_sentence = random.choice(train_sentences)\n",
    "print(f'Random sentences:\\n{random_train_sentence}')\n",
    "use_embedding_sentence = sentence_encoder_layer([random_train_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_embedding_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a model and fitting an NLP feature extraction from tensorflow hub\n",
    "inputs = tf.keras.layers.Input(shape=(), \n",
    "                               dtype=tf.string)\n",
    "x = sentence_encoder_layer(inputs) # tokenize text and create embedding of each sequence (512 long vector)\n",
    "x = tf.keras.layers.Dense(128, \n",
    "                          activation='relu')(x)\n",
    "# if you could add more layers here if you wanted to\n",
    "outputs = tf.keras.layers.Dense(num_classes, \n",
    "                                activation='softmax')(x)\n",
    "model_2 = tf.keras.Model(inputs, \n",
    "                         outputs, \n",
    "                         name='model_2_use_feature_extractor')\n",
    "model_2.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model_2_history = model_2.fit(train_dataset,\n",
    "                              epochs=3,\n",
    "                              steps_per_epoch=int(0.1 * len(train_dataset)),\n",
    "                              validation_data=val_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_pred_prob = model_2.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pred prob to classes\n",
    "model_2_preds = tf.argmax(model_2_pred_prob, axis=1)\n",
    "model_2_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_results = calculate_results(y_true=val_labels_label_encoded, y_pred=model_2_preds)\n",
    "\n",
    "model_results = pd.DataFrame({'model_0_results': model_0_results,\n",
    "                              'model_1_results': model_1_results,\n",
    "                              'model_2_results': model_2_results})\n",
    "model_results.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Conv1D with character embeending\n",
    "\n",
    "The paper  which we're replicationg states they used a combination of token and character-level embeddings.\n",
    "\n",
    "Previously we've token-level embeddings but we'll nedd to do similar steps for characters if we want to use char-level embeddings.\n",
    "\n",
    "see: https://medium.com/@WojtekFulmyk/text-tokenization-and-vectorization-in-nlp-ac5e3eb35b85\n",
    "\n",
    "see: https://www.kaggle.com/code/parulpandey/getting-started-with-nlp-feature-vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make functions to split sentences into characters\n",
    "def split_chars(text:str):\n",
    "    return ' '.join(list(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_preprocessor(text):\n",
    "    '''\n",
    "    Make text lowercase, remove text in square brackets,remove links,remove special characters\n",
    "    and remove words containing numbers.\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\",\" \",text) # remove special chars\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text splitting no-character-level sequence into characteres\n",
    "split_chars(random_train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sequence-level data splits into character-level data splits\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
    "\n",
    "train_chars[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the average character length?\n",
    "chars_lens = [len(sentence) for sentence in train_sentences]\n",
    "mean_chars_lens = np.mean(chars_lens)\n",
    "mean_chars_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution of our sequences at a character-level\n",
    "ax = plt.hist(chars_lens, bins=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find what character length cover 95% of sequence\n",
    "output_seq_char_len = int(np.percentile(chars_lens, 95))\n",
    "output_seq_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all character\n",
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHAR_TOKENS = len(alphabet) + 2 # add 2 for space and OOV token (OOV = out of vocab, '[UNK]')\n",
    "char_vectorizer = tf.keras.layers.TextVectorization(max_tokens=NUM_CHAR_TOKENS,\n",
    "                                                    output_sequence_length=output_seq_char_len,\n",
    "                                                    # standardize='lower_and_strip_punctuation', # set to None if want to leave punctuation in\n",
    "                                                    name='char_vectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(analyzer='char',\n",
    "                                    ngram_range=(2, 2), \n",
    "                                    strip_accents='ascii',\n",
    "                                    max_features=output_seq_char_len).fit(train_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer.get_feature_names_out()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = random.choice(train_chars)\n",
    "x = tf_idf_vectorizer.transform([choice]).toarray()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World level unigrams and bigrams\n",
    "count_vectorizer = CountVectorizer(stop_words=nltk.corpus.stopwords.words('english'),\n",
    "                                   preprocessor=custom_preprocessor, \n",
    "                                   ngram_range=(1,2),\n",
    "                                   min_df=2,\n",
    "                                   max_df=0.8)\n",
    "count_vectorizer.fit(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(count_vectorizer.vocabulary_)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character level bigrams\n",
    "count_vectorizer = CountVectorizer(preprocessor=custom_preprocessor,\n",
    "                                   stop_words=nltk.corpus.stopwords.words('english'),\n",
    "                                   ngram_range=(2,2),\n",
    "                                   max_features=output_seq_char_len,\n",
    "                                   strip_accents='ascii',\n",
    "                                   analyzer='char_wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = count_vectorizer.fit_transform([random.choice(train_chars)])\n",
    "train_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt character vectorizer to training\n",
    "char_vectorizer.adapt(train_chars)\n",
    "# test_vectors = count_vectorizer.transform(train_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check vocab stats\n",
    "char_vocab = char_vectorizer.get_vocabulary()\n",
    "print(f'Number of different characters in character vocab: {len(char_vocab)}')\n",
    "print(f'5 most common char: {char_vocab[:5]}')\n",
    "print(f'5 lear common char: {char_vocab[-5:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of different characters in character vocab: {len(list(count_vectorizer.vocabulary_))}')\n",
    "print(f'5 most common char: {list(count_vectorizer.vocabulary_)[:5]}')\n",
    "print(f'5 lear common char: {list(count_vectorizer.vocabulary_)[-5:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out character vectorizer\n",
    "random_train_chars = random.choice(train_chars)\n",
    "print(f'Charified text:\\n{random_train_chars}')\n",
    "print(f'\\nLength of character: {len(random_train_chars.split())}')\n",
    "\n",
    "vectorized_chars = char_vectorizer([random_train_chars])\n",
    "vectorized_chars_counter = count_vectorizer.fit_transform([random_train_chars])\n",
    "\n",
    "print(f'\\nVectorized Vector: {vectorized_chars}')\n",
    "# print(f'\\nLength of vectorized chars: {len(vectorized_chars[0])}')\n",
    "print(f'\\nShape of vectorized chars: {vectorized_chars.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Charified text:\\n{random_train_chars}')\n",
    "print(f'\\nLength of character: {len(random_train_chars.split())}')\n",
    "vectorized_chars_counter = count_vectorizer.fit_transform([random_train_chars])\n",
    "\n",
    "# https://stackoverflow.com/questions/12668027/good-ways-to-expand-a-numpy-ndarray\n",
    "vectorized_chars_counter_array = np.hstack((vectorized_chars_counter.toarray()[0], \n",
    "                                            np.zeros(output_seq_char_len - len(vectorized_chars_counter.toarray()[0])))).reshape(1, -1)\n",
    "print(f'\\nVectorized Counter: {vectorized_chars_counter_array}')\n",
    "# print(f'\\nLength of vectorized chars: {vectorized_chars_counter.getnnz()}')\n",
    "print(f'\\nShape of vectorized chars: {vectorized_chars_counter_array.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([len(char) for char in train_chars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile([len(sentence) for sentence in train_sentences], 95.57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(char_vocab) # alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_embed_keras = tf.keras.layers.Embedding(input_dim=len(char_vocab), # size of vocabulary\n",
    "                                             output_dim=25, # this is the size of the char embedding in the paper\n",
    "                                             mask_zero=True,\n",
    "                                             name='char_embed_keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Charified text:\\n{random_train_chars}')\n",
    "\n",
    "print(f'\\nCharified text:{len(random_train_chars.split())}')\n",
    "char_embed_keras_sample = char_embed_keras(char_vectorizer([random_train_chars]))\n",
    "\n",
    "print(f'\\nVectorized Vector: {char_embed_keras_sample}')\n",
    "print(f'\\nShape of vectorized chars: {char_embed_keras_sample.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see: https://towardsdatascience.com/word-embedding-techniques-word2vec-and-tf-idf-explained-c5d02e34d08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(random_train_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Conv1D model and fit with character-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Conv1D on chars\n",
    "inputs = tf.keras.layers.Input(shape=(1,), \n",
    "                               dtype=tf.string)\n",
    "x = char_vectorizer(inputs)\n",
    "x = char_embed_keras(x)\n",
    "x = tf.keras.layers.Conv1D(filters=64, \n",
    "                           kernel_size=5, \n",
    "                           padding='same', \n",
    "                           activation='relu')(x)\n",
    "# x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = tf.keras.layers.GlobalMaxPool1D()(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "outputs = tf.keras.layers.Dense(num_classes, \n",
    "                                activation='softmax')(x)\n",
    "model_3 = tf.keras.Model(inputs=inputs, \n",
    "                         outputs=outputs,\n",
    "                         name='model_3_conv1d_char_embedding')\n",
    "\n",
    "model_3.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, \n",
    "                                                         train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, \n",
    "                                                       val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_char_dataset = tf.data.Dataset.from_tensor_slices((test_chars, \n",
    "                                                        test_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_char_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model_3_history = model_3.fit(train_char_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_char_dataset)),\n",
    "                              epochs=3,\n",
    "                              validation_data=val_char_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_char_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.evaluate(val_char_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_preds_prob = model_3.predict(val_char_dataset)\n",
    "model_3_preds = tf.argmax(model_3_preds_prob, \n",
    "                          axis=1)\n",
    "\n",
    "model_3_results = calculate_results(y_true=val_labels_label_encoded, \n",
    "                                    y_pred=model_3_preds)\n",
    "\n",
    "model_results = pd.DataFrame({'model_0_results': model_0_results,\n",
    "                              'model_1_results': model_1_results,\n",
    "                              'model_2_results': model_2_results,\n",
    "                              'model_3_results': model_3_results})\n",
    "\n",
    "model_results.transpose().sort_values(by='accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Combine pretrained token embedding + characteres (hydrid)\n",
    "\n",
    "1. Create a token level embedding (similar model 1)\n",
    "2. create a character-level (similar model 3)\n",
    "3. Combine 1 & 2 model witch concatenate (layers.concatenate)\n",
    "4. Build a series of output layers on top  of 3 similar\n",
    "5. Construct a model which takes token and caracter-level sequences as input and produces sequence label probabilities as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup token inputs/model\n",
    "token_inputs =  tf.keras.layers.Input(shape=(), \n",
    "                                      dtype=tf.string, \n",
    "                                      name='token_input')\n",
    "\n",
    "token_embeddings = sentence_encoder_layer(token_inputs)\n",
    "\n",
    "token_output = tf.keras.layers.Dense(units=128, \n",
    "                                     activation='relu')(token_embeddings)\n",
    "\n",
    "token_model = tf.keras.Model(token_inputs, token_output, name='token_model')\n",
    "\n",
    "# 2. Setup character inputs/model\n",
    "char_inputs = tf.keras.Input(shape=(1,),\n",
    "                              dtype=tf.string,\n",
    "                              name='char_input')\n",
    "\n",
    "char_vectors = char_vectorizer(char_inputs)\n",
    "char_embeddings = char_embed_keras(char_vectors)\n",
    "char_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(25))(char_embeddings) # bi-LSTM shown in figure 1 of https://arxiv.org/abs/1710.06071\n",
    "char_model = tf.keras.Model(inputs=char_inputs,\n",
    "                            outputs=char_bi_lstm,\n",
    "                            name='char_model')\n",
    "\n",
    "# 3. Setup concatenate token and char inputs/model\n",
    "token_char_concat = tf.keras.layers.Concatenate(name='token_char_hydrid')([token_model.output, \n",
    "                                                                           char_model.output])\n",
    "\n",
    "# 4. create output layers - adding in Dropout, discussed in section 4.2 of https://arxiv.org/abs/1710.06071\n",
    "combined_dropout = tf.keras.layers.Dropout(0.5)(token_char_concat)\n",
    "combined_dense = tf.keras.layers.Dense(128, \n",
    "                                       activation='relu')(combined_dropout)\n",
    "final_dropout = tf.keras.layers.Dropout(0.6)(combined_dense)\n",
    "output_layer = tf.keras.layers.Dense(num_classes, activation='softmax')(final_dropout)\n",
    "\n",
    "# 5. Construct model with char and token input/model\n",
    "model_4 = tf.keras.Model(inputs=[token_model.input, char_model.input], outputs=output_layer, name='model_4_token_and_char_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model\n",
    "plot_model(model_4, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "# model_4.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model_4.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining token and character data into tf.data dataset\n",
    "\n",
    "see: https://www.tensorflow.org/api_docs/python/tf/data/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine chars and token\n",
    "train_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, \n",
    "                                                            train_chars)) # make data\n",
    "train_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # make labels\n",
    "train_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, \n",
    "                                                train_char_token_labels)) # combile data and labels\n",
    "train_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # prefetch and batch dataset\n",
    "\n",
    "val_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, \n",
    "                                                          val_chars))\n",
    "val_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, \n",
    "                                              val_char_token_labels))\n",
    "val_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # prefetch and batch dataset\n",
    "\n",
    "test_char_token_data = tf.data.Dataset.from_tensor_slices((test_sentences, \n",
    "                                                           test_chars))\n",
    "test_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "test_char_token_dataset = tf.data.Dataset.zip((test_char_token_data, \n",
    "                                               test_char_token_labels))\n",
    "test_char_token_dataset = test_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # prefetch and batch dataset\n",
    "\n",
    "\n",
    "train_char_token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model 4\n",
    "# fit the model\n",
    "model_4_history = model_4.fit(train_char_token_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_char_token_dataset)),\n",
    "                              epochs=5,\n",
    "                              validation_data=val_char_token_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_char_token_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4_preds_prob = model_4.predict(val_char_token_dataset)\n",
    "model_4_preds = tf.argmax(model_4_preds_prob, \n",
    "                          axis=1)\n",
    "\n",
    "model_4_results = calculate_results(y_true=val_labels_label_encoded, \n",
    "                                    y_pred=model_4_preds)\n",
    "\n",
    "model_results = pd.DataFrame({'model_0_results': model_0_results,\n",
    "                              'model_1_results': model_1_results,\n",
    "                              'model_2_results': model_2_results,\n",
    "                              'model_3_results': model_3_results,\n",
    "                              'model_4_results': model_4_results})\n",
    "\n",
    "model_results.transpose().sort_values(by='accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5: Pretrained token embeddings + character embeddings + positional embeddings\n",
    "\n",
    "## Feature Engineering\n",
    "* Taking `non-obvious features` from the data and encoding them numerically to help our model learn\n",
    "* How can we add extra sources of data to our model?\n",
    "\n",
    "Data augmentation is a form of feature engineering\n",
    "\n",
    "> **Note**: Any engineered features used to train a model need to be available at test time.  \n",
    "> In our case, line numbers and total line are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many different line numbers are there?\n",
    "train_df['number'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of number\n",
    "train_df.number.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tensorflow one-hot-encoder to create tensors of our number column\n",
    "train_line_numbers_one_hot = tf.one_hot(train_df['number'].to_numpy(), depth=15) # depth=15 to prevent large dimension \n",
    "val_line_numbers_one_hot = tf.one_hot(val_df['number'].to_numpy(), depth=15) # depth=15 to prevent large dimension \n",
    "test_line_numbers_one_hot = tf.one_hot(test_df['number'].to_numpy(), depth=15) # depth=15 to prevent large dimension \n",
    "\n",
    "train_line_numbers_one_hot[:10], train_line_numbers_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've encoded our line numbers feature. let's do the same for our total lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['total_line'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of total lines\n",
    "\n",
    "train_df['total_line'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the coverage of a total lines\n",
    "np.percentile(train_df.total_line, 98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Tensorflow to create one-hot-encoder tensors of our total_line feature\n",
    "train_total_line_one_hot = tf.one_hot(train_df.total_line.to_numpy(), depth=20)\n",
    "val_total_line_one_hot = tf.one_hot(val_df.total_line.to_numpy(), depth=20)\n",
    "test_total_line_one_hot = tf.one_hot(test_df.total_line.to_numpy(), depth=20)\n",
    "\n",
    "train_total_line_one_hot.shape, train_total_line_one_hot[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a tribrid embedding model\n",
    "\n",
    "1. create a token-level model\n",
    "2. create a character-level model\n",
    "3. create a model for the line number feature\n",
    "4. create a model for the total line feature\n",
    "5. combine the outputs of 1 and 2 using tf.keras.layers.Concatenate\n",
    "6. combine the outputs of 3, 4, 5 using tf.keras.layers.Concatenate\n",
    "7. create an output layer to accept the tribrided embedding and output label probabilities\n",
    "8. combine the inputs of 1, 2, 3, 4 and outputs of into a tf.keras.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line_numbers_one_hot[0].shape, train_total_line_one_hot[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Token inputs\n",
    "token_inputs = tf.keras.layers.Input(shape=(), \n",
    "                                     dtype=tf.string, \n",
    "                                     name='token_input')\n",
    "\n",
    "# 1.1 create token embedding with pretrained model\n",
    "token_embeddings = sentence_encoder_layer(token_inputs) # transfer learning - pretrained model \n",
    "\n",
    "# 1.2 token output with activation relu\n",
    "token_outputs = tf.keras.layers.Dense(units=128, \n",
    "                                      activation='relu')(token_embeddings)\n",
    "\n",
    "# 1.3 create token model\n",
    "token_model = tf.keras.Model(inputs=token_inputs,\n",
    "                             outputs=token_outputs,\n",
    "                             name='token_model')\n",
    "\n",
    "# 2. Char inputs\n",
    "char_inputs = tf.keras.layers.Input(shape=(1,), \n",
    "                                    dtype=tf.string, \n",
    "                                    name='char_input')\n",
    "\n",
    "# 2.1 create a character vectorizer\n",
    "char_vectors = char_vectorizer(char_inputs)\n",
    "\n",
    "# 2.2 create a character embeddings with keras Embedding output_dim\n",
    "char_embeddings = char_embed_keras(char_vectors)\n",
    "\n",
    "# 2.3 create bidirectional layer with LSTM\n",
    "char_bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(24))(char_embeddings) # bi-LSTM shown in figure 1 of https://arxiv.org/abs/1710.06071\n",
    "\n",
    "# 2.3 create a char model\n",
    "char_model = tf.keras.Model(inputs=char_inputs,\n",
    "                            outputs=char_bi_lstm,\n",
    "                            name='char_model')\n",
    "\n",
    "# 3. Line number model\n",
    "line_number_inputs = tf.keras.layers.Input(shape=(15,), # train_line_numbers_one_hot[0].shape\n",
    "                                           dtype=tf.float32,\n",
    "                                           name='line_number_input')\n",
    "\n",
    "# 3.1 create a dense layer with 32 units\n",
    "line_number_outputs = tf.keras.layers.Dense(units=32,\n",
    "                                            activation='relu')(line_number_inputs)\n",
    "\n",
    "# 3.2 create a line model\n",
    "line_number_model = tf.keras.Model(inputs=line_number_inputs,\n",
    "                                   outputs=line_number_outputs,\n",
    "                                   name='line_number_model')\n",
    "# 4. Total lines model\n",
    "total_number_inputs = tf.keras.layers.Input(shape=(20,), # train_total_line_one_hot[0].shape\n",
    "                                            dtype=tf.float32,\n",
    "                                            name='total_number_input')\n",
    "\n",
    "# 4.1 create a dense layer with 32 units and activation relu\n",
    "total_number_outputs = tf.keras.layers.Dense(units=32,\n",
    "                                             activation='relu')(total_number_inputs)\n",
    "\n",
    "# 4.2 create a total model\n",
    "total_number_model = tf.keras.Model(inputs=total_number_inputs,\n",
    "                                    outputs=total_number_outputs,\n",
    "                                    name='total_number_model')\n",
    "\n",
    "# 5. combine token and char embeddings into hydrid embedding\n",
    "combined_embeddings = tf.keras.layers.Concatenate(name='char_token_hydrid_embedding')([token_model.output, \n",
    "                                                                                       char_model.output])\n",
    "# 5.1 create a dense layer with 256 unis and activation relu\n",
    "z = tf.keras.layers.Dense(units=256, \n",
    "                          activation='relu')(combined_embeddings)\n",
    "\n",
    "# 5.2 create a Dropout layer with 0.5\n",
    "z = tf.keras.layers.Dropout(0.5)(z)\n",
    "\n",
    "# 6. combine positional embedding with conbinaed token and char embeddings\n",
    "tribrid_embeddings = tf.keras.layers.Concatenate(name='char_token_possitional_embedding')([line_number_model.output,\n",
    "                                                                                          total_number_model.output,\n",
    "                                                                                          z])\n",
    "\n",
    "# 7. create output layer \n",
    "output_layer = tf.keras.layers.Dense(units=num_classes,\n",
    "                                     activation='softmax',\n",
    "                                     name='output_layer')(tribrid_embeddings)\n",
    "\n",
    "# 8. put together model with all kinds of inputs\n",
    "model_5 = tf.keras.Model(inputs=[line_number_model.input,\n",
    "                                 total_number_model.input,\n",
    "                                 token_model.input,\n",
    "                                 char_model.input],\n",
    "                                 outputs=output_layer,\n",
    "                                 name='model_5_tribrid')\n",
    "\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model 5 to explore visualize\n",
    "plot_model(model_5, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_5.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), # helps to prevent overfitting\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tribrid embedding datasets using tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and validation datasets (with all for kinds of inputs data) must be the same to the inputs model order\n",
    "train_char_token_pos_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot,\n",
    "                                                                train_total_line_one_hot,\n",
    "                                                                train_sentences,\n",
    "                                                                train_chars))\n",
    "\n",
    "train_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_char_token_pos_dataset = tf.data.Dataset.zip((train_char_token_pos_data, train_char_token_pos_labels))\n",
    "train_char_token_pos_dataset = train_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_char_token_pos_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot,\n",
    "                                                                train_total_line_one_hot,\n",
    "                                                                train_sentences,\n",
    "                                                                train_chars))\n",
    "# do the same for validation data\n",
    "val_char_token_pos_data = tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot,\n",
    "                                                              val_total_line_one_hot,\n",
    "                                                              val_sentences,\n",
    "                                                              val_chars))\n",
    "val_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_char_token_pos_dataset = tf.data.Dataset.zip((val_char_token_pos_data, val_char_token_pos_labels))\n",
    "val_char_token_pos_dataset = val_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# do the same for test data\n",
    "test_char_token_pos_data = tf.data.Dataset.from_tensor_slices((test_line_numbers_one_hot,\n",
    "                                                                test_total_line_one_hot,\n",
    "                                                                test_sentences,\n",
    "                                                                test_chars))\n",
    "test_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_char_token_pos_dataset = tf.data.Dataset.zip((test_char_token_pos_data, test_char_token_pos_labels))\n",
    "test_char_token_pos_dataset = test_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_char_token_pos_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model 5\n",
    "model_5_history = model_5.fit(train_char_token_pos_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_char_token_pos_dataset)),\n",
    "                              epochs=3,\n",
    "                              validation_data=val_char_token_pos_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_char_token_pos_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5_preds_prob = model_5.predict(val_char_token_pos_dataset)\n",
    "model_5_preds = tf.argmax(model_5_preds_prob, \n",
    "                          axis=1)\n",
    "\n",
    "model_5_results = calculate_results(y_true=val_labels_label_encoded, \n",
    "                                    y_pred=model_5_preds)\n",
    "\n",
    "model_results = pd.DataFrame({'model_0_results': model_0_results,\n",
    "                              'model_1_results': model_1_results,\n",
    "                              'model_2_results': model_2_results,\n",
    "                              'model_3_results': model_3_results,\n",
    "                              'model_4_results': model_4_results,\n",
    "                              'model_5_results': model_5_results})\n",
    "\n",
    "model_results.transpose().sort_values(by='accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and compare all models results\n",
    "model_results.plot(kind='bar', figsize=(20, 5)).legend(bbox_to_anchor=(1.0,  1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5.save(f'{MODEL_PATH}/skimlist_tribrid_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = tf.keras.models.load_model(f'{MODEL_PATH}/skimlist_tribrid_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded_preds_prob = model_loaded.predict(val_char_token_pos_dataset)\n",
    "model_loaded_preds = tf.argmax(model_loaded_preds_prob, \n",
    "                          axis=1)\n",
    "\n",
    "model_loaded_results = calculate_results(y_true=val_labels_label_encoded, \n",
    "                                    y_pred=model_loaded_preds)\n",
    "\n",
    "model_results = pd.DataFrame({'model_0_results': model_0_results,\n",
    "                              'model_1_results': model_1_results,\n",
    "                              'model_2_results': model_2_results,\n",
    "                              'model_3_results': model_3_results,\n",
    "                              'model_4_results': model_4_results,\n",
    "                              'model_5_results': model_5_results,\n",
    "                              'model_5_loaded_results': model_loaded_results})\n",
    "\n",
    "model_results.transpose().sort_values(by='accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features=25)\n",
    "vectorized_chars_counter = count_vectorizer.fit_transform([random_train_chars])\n",
    "vectorized_chars_counter_array = np.hstack((vectorized_chars_counter.toarray()[0], \n",
    "                                            np.zeros(output_seq_char_len - len(vectorized_chars_counter.toarray()[0])))).reshape(1, -1)\n",
    "\n",
    "vectorized_chars_counter_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(max_features=25,\n",
    "                                    stop_words=nltk.corpus.stopwords.words('english'),\n",
    "                                    strip_accents='ascii',\n",
    "                                    analyzer='char_wb')\n",
    "char_embed_tf_idf = tf_idf_vectorizer.fit_transform(np.hstack((random_train_chars.split(), \n",
    "                                                               np.zeros(output_seq_char_len - len(random_train_chars.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_embed_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "vocabs = np.hstack((random_train_chars.split(),\n",
    "                    np.zeros(output_seq_char_len - len(random_train_chars.split()))))\n",
    "w2v_model = Word2Vec(min_count=4,\n",
    "                     window=4,\n",
    "                     vector_size=len(vocabs),\n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     sg = 1,\n",
    "                     workers=cores-1)\n",
    "w2v_model.build_vocab(vocabs, progress_per=10000)\n",
    "w2v_model.train(vocabs, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('D:/txt/cbow_s1000.txt', 'r', encoding='utf-8') as f:\n",
    "#     lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line in lines[1:10]:\n",
    "#     print(line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('input', help='Single embedding txt file')\n",
    "# parser.add_argument('output', help='Output basename without extension')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# embedding_cbow_file = args.ouput + '.npy'\n",
    "# vocabulary_cbow_file = args.output + '.txt'\n",
    "\n",
    "# https://blog.ekbana.com/loading-glove-pre-trained-word-embedding-model-from-text-file-faster-5d3e8f2b8455\n",
    "# https://gist.github.com/erickrf/e54cd0f3d917ec61b3ae758a5e47b883\n",
    "# https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "# https://medium.com/@zafaralibagh6/a-simple-word2vec-tutorial-61e64e38a6a1\n",
    "# https://github.com/deeplearning4j\n",
    "# https://medium.com/@erkajalkumari/step-by-step-guide-to-word2vec-with-gensim-2c4cd9dde01f\n",
    "\n",
    "# vocabs = []\n",
    "# words_vector = []\n",
    "# for line in lines[1:100]:\n",
    "#     splitlines = line.split()\n",
    "#     vocabs.append(splitlines[0].strip())\n",
    "#     words_vector.append(np.fromiter((np.float32(x.replace(',', '.')) for x in splitlines[1:]), dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('D:/txt/cbow_1000.npy', np.array(words_vector))\n",
    "# with open('D:/txt/cbow_1000.vocab', 'w', encoding='utf-8') as f:\n",
    "#     for vocab in vocabs:\n",
    "#         f.write(vocab)\n",
    "#         f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary(input_txt, output_vocab, output_binary):\n",
    "    \"\"\"\n",
    "    :param input_txt - takes path of embedding which is in text format\n",
    "    :param output_vocab - output vocabulary\n",
    "    :param output_binary - output numpy array binary \n",
    "    :return a binary file Numpy .npy format\n",
    "    \"\"\"\n",
    "    with open(input_txt, 'r', encoding='utf-8') as read_file:\n",
    "        lines = read_file.readlines()\n",
    "        word_vector = []\n",
    "        with open(output_vocab, 'w', encoding='utf-8') as write_file:\n",
    "            for line in lines[1:100]:\n",
    "                splitlines = line.split()\n",
    "                write_file.write(splitlines[0].strip().encode('utf-8'))\n",
    "                write_file.write(\"\\n\")\n",
    "                word_vector.append(np.fromiter((np.float32(x.replace(',', '.')) for x in splitlines[1:]), dtype=np.float32))\n",
    "    np.save(output_binary, np.array(word_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_binary():\n",
    "    \"\"\"\n",
    "    encoding='cp1252'\n",
    "    It loads embedding provided by glove which is saved as binary file. Loading of this model is\n",
    "    about  second faster than that of loading of txt glove file as model.\n",
    "    :param embeddings_path: path of glove file.\n",
    "    :return: glove model\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "\n",
    "    with open('D:/txt/cbow_1000.vocab', 'r', encoding='utf-8') as file:\n",
    "        words = [line.strip() for line in file]\n",
    "    \n",
    "    wv = np.load('D:/txt/cbow_1000.npy')\n",
    "    \n",
    "    for i, w in enumerate(words):\n",
    "        model[w] = wv[i]\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_phrases(sentences):\n",
    "    phrases = Phrases(sentences,\n",
    "                      min_count=5,\n",
    "                      threshold=7,\n",
    "                      progress_per=1000)\n",
    "    return Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = build_phrases([\"hoje é bonito\", \"amanhã será feito\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_w2v = load_embeddings_binary()\n",
    "# embedding_df = pd.DataFrame(dict_w2v)\n",
    "# embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v(sentence, model):\n",
    "    \"\"\"\n",
    "    :param sentence: inputs a single sentences whose word embedding is to be extracted.\n",
    "    :param model: inputs glove model.\n",
    "    :return: returns numpy array containing word embedding of all words    in input sentence.\n",
    "    \"\"\"\n",
    "    return np.array([model.get(val, np.zeros(1000)) for val in sentence.split()], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_sentences = [\"oi me ferrei!\"]\n",
    "model_w2v = Word2Vec(sentences=[sentence.split() for sentence in w2v_sentences], \n",
    "                     vector_size=1000,\n",
    "                     min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.build_vocab([sentence.split() for sentence in w2v_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.wv.get_normed_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.wv.get_vector('ferrei!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.wv.similar_by_vector(model_w2v.wv.get_vector('oi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v = KeyedVectors.load_word2vec_format('D:/txt/cbow_s1000.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v.add_vector(\"ferrei!\", model_w2v.wv.get_vector('ferrei!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ [w2v.get_vector(w) for w in sentence.split()] for sentence in [\"hoje é um bom dia\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_w2v.update({'ferrei!': model_w2v.wv.get_vector('ferrei!')})\n",
    "# result = get_w2v(\"eu me ferrei!\", dict_w2v)\n",
    "# result.shape, result[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_to_binary('D:/txt/cbow_s1000.txt', 'D:/txt/cbow_1000.vocab', 'D:/txt/cbow_1000.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
