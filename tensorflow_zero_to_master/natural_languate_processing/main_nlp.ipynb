{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) with Tensorflow\n",
    "\n",
    "## What we're goint to cover?\n",
    "\n",
    "- Downloading and preparing a text dataset\n",
    "- How to prepare text dataset for modelling (`tokenization embedding`)\n",
    "- Setting up multiple modelling experiments with recurrent neural networking (RNNs)\n",
    "- Build a text `feature extraction model using Tensorflow Hub`\n",
    "- Finding the most wrong predictions examples\n",
    "- Using a model we've built to make predictions on text from the wild\n",
    "\n",
    "see: https://en.wikipedia.org/wiki/Recurrent_neural_network  \n",
    "see: https://awari.com.br/deep-learning-rnn-utilizando-redes-rnn-recurrent-neural-networks-no-deep-learning/\n",
    "\n",
    "RNN - Recurrent Neural Network  \n",
    "LSTM - Long short-term memory\n",
    "\n",
    "how to add deep learning model in android application: https://www.youtube.com/watch?v=tySgZ1rEbW4&t=987s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hyperparameter/Layer Type | What does it do?                                               | Typical Value                                                                        |\n",
    "| ------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------ |\n",
    "| Input (txt)               | Target text/sequencial you'd like to discover pattern in       | whatever you can represent as text or a sequence                                     |\n",
    "| Input Layer               | Takes target in sequence                                       | input_shape = [batch_size, embedding_size] or [batch_size, sequence_shape]           |\n",
    "| Text Vectorization Layer  | Maps input sequences to number                                 | tf.keras.layer.experimental.preprocessing.TextVectorization                          |\n",
    "| Embedding                 | mapping of text vectors to embedding matrix (how words relate) | tf.keras.layers.Embedding                                                            |\n",
    "| RNN                       | Finds patterns in sequence                                     | SimpleRNN, LSTM, GRU                                                                 |\n",
    "| Hidden activation         | Adds non-linearity to learned features (non-streigh-line)      | Usually Tanh (hyperbolic tangent) tf.keras.activations.tanh                          |\n",
    "| Pooling Layer             | Reduces the dimensionality of learned sequence (ConvD1)        | tf.keras.layers.GlobalAlveragePooling1D or tf.keras.layers.GlobalMaxPool1D           |\n",
    "| Full connected Layer      | Futher refines learned features from recurrent layers          | tf.keras.layers.Dense                                                                |\n",
    "| Output Layer              | Takes learned features outputs them in shape of target labels  | output_shape = number_of_classes                                                     |\n",
    "| Output Activation         | Adds non-linearities to output layer                           | tf.keras.activations.sigmoid (binary classification) or tf.keras.activations.softmax |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# create LSTM Model\n",
    "inputs = layers.Input(shape=(1,), dtype='string')\n",
    "x = text_vectorizer(inputs) # turn input sequence to number\n",
    "x = embedding(x) # create embedding matrix\n",
    "x = layers.LSTM(64, activation='tanh')(x) # return vector for whole sequence\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(train_sequences, train_labels, epochs=5)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# add path root project to read helper fuctions\n",
    "sys.path.append(os.path.join('../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "tfhub_url = 'https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2'\n",
    "\n",
    "# create a Keras Layer using the pretrained layer from tensorflow hub\n",
    "sentence_encoder_layer = hub.KerasLayer(tfhub_url, \n",
    "                                        input_shape=[], \n",
    "                                        dtype=tf.string,\n",
    "                                        trainable=False, \n",
    "                                        name='USE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "In past modules, we've created a brunch of helper functions to do small task required for our notebooks. Rather than rewrite all these, we can import a script and load them in from threre. The script we've got available can be found on github: https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request as ur\n",
    "# uncomment this line below and run it to download helper_functions file\n",
    "# ur.urlretrieve('https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py', filename='helper_functions.py')\n",
    "from helper_functions import create_tensorboard_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Dataset\n",
    "\n",
    "The dataset we're going to be using is Kaggle's introduction to NLP dataset\n",
    "\n",
    "see: https://www.kaggle.com/competitions/nlp-getting-started  \n",
    "see: https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE = os.path.join('../../', 'storage')\n",
    "IMAGE_PATH = f'{STORAGE}/images'\n",
    "ZIP_PATH = f'{STORAGE}/zip'\n",
    "MODEL_PATH = f'{STORAGE}/models'\n",
    "NLP = f'{STORAGE}/nlp'\n",
    "\n",
    "# concat paths\n",
    "LIST_PATHS = [IMAGE_PATH, ZIP_PATH, MODEL_PATH, NLP]\n",
    "\n",
    "# create a directory to save tensorboard logs\n",
    "tensorboard_logs = f'{NLP}/tensorboard/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm dir create\n",
    "os.listdir(STORAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in LIST_PATHS:\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'nlp_getting_started.zip'\n",
    "url = f'https://storage.googleapis.com/ztm_tf_course/{filename}'\n",
    "folder = filename.split('.')[0]\n",
    "\n",
    "# unzip_data(url, filename, ZIP_PATH, NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a text dataset\n",
    "\n",
    "see: https://www.w3schools.com/python/python_file_open.asp  \n",
    "see: https://www.tensorflow.org/tutorials/load_data/text?hl=pt-br\n",
    "\n",
    "To visualize our text samples, we first have to read them in, one way to do would be to use python read, but we prefer to get visual straight away. So another way to do this is to sue pandas library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(f'{NLP}/train.csv')\n",
    "test_df = pd.read_csv(f'{NLP}/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many examples of each class?\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification when data is imbalanced\n",
    "\n",
    "see: https://www.tensorflow.org/tutorials/structured_data/imbalanced_data?hl=pt-br\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many total samples\n",
    "train_df.shape[0], test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize some random training examples\n",
    "import random\n",
    "\n",
    "random_index = random.randint(0, train_df.shape[0] - 5) # create random indexes not higher than total of samples\n",
    "for row in train_df_shuffled[['text', 'target']][random_index:random_index+5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f'Target: {target}', '(real disaster)' if target > 0 else '(not real disaster)')\n",
    "    print(f'Text: {text}')\n",
    "    print('----\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and validation sets\n",
    "\n",
    "see: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# use train_test_split to split training data and validation sets\n",
    "train_sequences, val_sequences, train_labels, val_labels = train_test_split(train_df_shuffled['text'].to_numpy(),\n",
    "                                                                           train_df_shuffled['target'].to_numpy(),\n",
    "                                                                           test_size=0.1, # use 10% of training data validation\n",
    "                                                                           random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the lengh\n",
    "train_sequences.shape[0], train_labels.shape[0], val_sequences.shape[0], val_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first 5 samples\n",
    "train_sequences[:5], train_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting text into numbers\n",
    "\n",
    "see: https://www.tensorflow.org/text/guide/word_embeddings?hl=pt-br\n",
    "\n",
    "When dealing with a text problem, one of the first things you'll have to do before you can build a model is to convert your text to numbers.\n",
    "\n",
    "There are a few ways to do this, namely:\n",
    "\n",
    "- `Tokenization`: - direct mapping of token (a token could be a word or a character) to number\n",
    "\n",
    "- `Embedding`: create a matrix of feature vector for each token (the size of the feature vector can be defined and this embedding can be learned)\n",
    "\n",
    "```python\n",
    "\n",
    "# I Love Tensorflow\n",
    "\n",
    "                [[1, 0, 0],\n",
    "OneHotEncoder = [0, 1, 0],\n",
    "                [0, 0, 1]]\n",
    "\n",
    "            [[0.234, 0.2323, 0.34],\n",
    "Embedding = [0.343, 0.222, 0.333],\n",
    "            [0.111, 0.343, 0.999]]\n",
    "```\n",
    "\n",
    "`Tokenization`: straight mapping from token to number (can be modelled but quick get too big)  \n",
    "`Embedding`: richer representation of relationship between tokens (can limit size + can be learned)\n",
    "\n",
    "```mermaid\n",
    "graph TD;\n",
    "    A[I Love Tensorflow] -->  Number[0 1 2]\n",
    "    A[I Love Tensorflow] --> OneHotEncoder[1, 0, 0\n",
    "                                0, 1, 0\n",
    "                                0, 0, 1 ]\n",
    "    A[I Love Tensorflow] --> Embedding[0.234, 0.2323, 0.34\n",
    "                                0.343, 0.222, 0.333,\n",
    "                                0.111, 0.343, 0.999]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# use the default TextVectorization parameters\n",
    "text_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=None, # how many words in the vocabulary (automatically add <OOV>)\n",
    "                                                                               standardize='lower_and_strip_punctuation',\n",
    "                                                                               split='whitespace',\n",
    "                                                                               ngrams=None, # create groups of n-words?\n",
    "                                                                               output_mode='int', # how to map tokens to number\n",
    "                                                                               output_sequence_length=None, # how long do you want your sequence to be\n",
    "                                                                               pad_to_max_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization (Tokenization)\n",
    "\n",
    "see: https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization  \n",
    "see: https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/  \n",
    "see: https://en.wikipedia.org/wiki/Tf%E2%80%93idf  \n",
    "see: https://monkeylearn.com/blog/what-is-tf-idf/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_sequences[2].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(sum([len(i.split()) for i in train_sequences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average number of tokens (words) in the training tweets\n",
    "# WARNING:\n",
    "# this value is very important to final of processing\n",
    "round(sum([len(i.split()) for i in train_sequences])/len(train_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup text vectorization variables\n",
    "max_vocab_length = 10000 # max number of words to have in our vacabulary\n",
    "max_length = 15 # max length our sequences will be (e.g how many words from a Tweet does a model see?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=max_vocab_length,\n",
    "                                                                               standardize='lower_and_strip_punctuation',\n",
    "                                                                               split='whitespace',\n",
    "                                                                               ngrams=None,\n",
    "                                                                               output_mode='int',\n",
    "                                                                               output_sequence_length=max_length,\n",
    "                                                                               pad_to_max_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample sequence and tokenize it\n",
    "sample_sequence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose random sentence from the training dataset and tokenize it \n",
    "random_sentence = random.choice(train_sequences)\n",
    "print(f'Original Text:\\n{random_sentence}\\n\\nVectorized: {text_vectorizer(random_sentence)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary() # get all the unique words in \n",
    "top_5_words = words_in_vocab[:5] # get the most common  words\n",
    "bottom_5_words = words_in_vocab[-5:]  # get the least common words\n",
    "\n",
    "print(f'Number of words in vocabulary: {len(words_in_vocab)}')\n",
    "print(f'5 most common words: {top_5_words}')\n",
    "print(f'5 least common words: {bottom_5_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Embedding using an Embedding Layer\n",
    "\n",
    "To make our embedding we going to use tensorflow's embedding layer: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "\n",
    "The parameters we care most about for our embedding layer:\n",
    "\n",
    "- `input_dim`: the size of our vocabulary\n",
    "- `output_dim`: the size of the output embedding vector for example, a value fo 100 would mean each token gets represented by a vector 100 long\n",
    "- `input_length`: length of the sequences being passed to the embedding layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.embedding.Embedding at 0x2c5fd410a58>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = tf.keras.layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
    "                                      output_dim=128,\n",
    "                                      embeddings_initializer='uniform',\n",
    "                                      input_length=max_length, # how long is each input\n",
    "                                      )\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose random sentence from the training dataset and tokenize it \n",
    "random_sentence = random.choice(train_sequences)\n",
    "print(f'Original Text:\\n{random_sentence}\\n\\nEmbedding:\\n{embedding(text_vectorizer([random_sentence]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embedding = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embedding[0][0], random_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling a text dataset (running a series of experiments)\n",
    "\n",
    "see: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "0. Model: Naive Bayes with `TF-IDF` encoder (baseline)\n",
    "1. Model: Feed Forward Neural Network (dense model)\n",
    "2. Model: `LTSM` (RNN) Long-Short Term Memory Recurrent Neural Network\n",
    "3. Model: `GRU` (RNN) Gated Recurrent Unit\n",
    "4. Model: Bidirection `LTSM` (RNN)\n",
    "5. Model: 1D Convolutional Neural Network\n",
    "6. Model: Tensorflow Hub Pretrained Feature Extractor\n",
    "7. Model: Tensorflow Hub Pretrained Feature Extractor (10% of data)\n",
    "\n",
    "Now we've a got way to turn our text sequences into numbers, it's time to start building a series of modelling experiments. We'll start with a baseline and move on from here.\n",
    "\n",
    "How are we going to approach all of these?\n",
    "\n",
    "- Create a model\n",
    "- Build a model\n",
    "- Fit a model\n",
    "- Evaluate a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0: Getting a baseline\n",
    "\n",
    "As with all machine learning modelling experiments, it's important to create a baseline model so you've got a benchmark for future experiments to build upon.\n",
    "\n",
    "To create our baseline, we'll use Sklearn's Multinomial Naive Bayes using the TF-IDF formula to convert our words to numbers.\n",
    "\n",
    "> **Note**: It's common practice to use non-DL (non-deep-learning) algorithms as a baseline because of their speed and then later using DL to see if you can improve upon them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tokenization and modelling pipeline\n",
    "model_0 = Pipeline([ \n",
    "    ('tfidf', TfidfVectorizer()),  # convert words to number using tf-idf\n",
    "    ('clf', MultinomialNB()) # model text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to training data\n",
    "model_0.fit(train_sequences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our baseline model achieves an accuracy of: 79.27%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our baseline model\n",
    "baseline_score = model_0.score(val_sequences, val_labels)\n",
    "print(f'Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions\n",
    "baseline_preds = model_0.predict(val_sequences)\n",
    "baseline_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to evaluate: accuracy, precision, recall, f1-score\n",
    "# see: https://stackoverflow.com/questions/3490738/how-to-sum-dict-elements\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from functools import reduce\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Model accuracy, precision, recal and f1-score of a binary classification model.\n",
    "    \"\"\"\n",
    "    # calculate accuracy\n",
    "    m_accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # calculate model precision, recall and f1-score using 'weighted' average\n",
    "    m_precision, m_recall, m_f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    results = {\n",
    "        'accuracy':     m_accuracy,\n",
    "        'precision':    m_precision,\n",
    "        'recall':       m_recall,\n",
    "        'f1':           m_f1_score}\n",
    "    \n",
    "    return reduce(lambda x, y: dict((k, v * 100) for k, v in results.items()), results)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get baseline results\n",
    "baseline_results = calculate_results(y_true=val_labels, y_pred=baseline_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model_0_results</th>\n",
       "      <td>79.265092</td>\n",
       "      <td>78.621898</td>\n",
       "      <td>81.1139</td>\n",
       "      <td>79.265092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  accuracy         f1  precision     recall\n",
       "model_0_results  79.265092  78.621898    81.1139  79.265092"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results = pd.DataFrame({'model_0_results': baseline_results})\n",
    "model_results.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: A simple dense model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model with functional api\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string) # inputs are 1-dimensional strings\n",
    "x = text_vectorizer(inputs) # turn the input text into numbers\n",
    "x = embedding(x) # create an embedding of the numberzed inputs\n",
    "# x = tf.keras.layers.GlobalAveragePooling1D()(x) # condense the feature vector for each token to one vector\n",
    "x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "# x = tf.keras.layers.GlobalAvgPool1D()(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x) # create the output layer, want binary outputs so use sigmoid activation functions\n",
    "model_1 = tf.keras.Model(inputs, outputs, name='mode_1_dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_history = model_1.fit(x=train_sequences,\n",
    "                              y=train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sequences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(dir_name=tensorboard_logs, experiment_name='model_1_dense')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "# GlobalAveragePooling1D    ===>>>      [0.491241455078125, 0.7900262475013733]\n",
    "# GlobalMaxPooling1D        ===>>>      [0.536510705947876, 0.7952755689620972]\n",
    "# GlobalAvgPool1D           ===>>>      [0.48092105984687805, 0.7821522355079651]\n",
    "model_1.evaluate(val_sequences, val_labels)\n",
    "model_1_pred_probs = model_1.predict(val_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a single prediction\n",
    "model_1_pred_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first 5\n",
    "model_1_pred_probs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert model prediction probabilities to label format\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\n",
    "model_1_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate our mode_1 results\n",
    "model_1_results = calculate_results(y_true=val_labels, y_pred=model_1_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = pd.DataFrame({'model_0_results': baseline_results,\n",
    "                              'model_1_results': model_1_results})\n",
    "\n",
    "model_results.transpose().sort_values(by='f1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing learned embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vocabulary from the text vectorization layer\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "len(words_in_vocab), words_in_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the weight matrix of embedding layer\n",
    "# (these are the numerical representations of each token in our training data)\n",
    "embed_weights = model_1.get_layer('embedding').get_weights()\n",
    "print(embed_weights[0].shape) # same size as vocab size and embedding_dim (output_dim of our embedding layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding files (we got this from tensorflow's word embedding documentation)\n",
    "# see: https://www.tensorflow.org/text/guide/word_embeddings?hl=pt-br\n",
    "import io\n",
    "\n",
    "out_v = io.open(f'{NLP}/vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open(f'{NLP}/metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(words_in_vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = embed_weights[0][index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (RNN's)\n",
    "\n",
    "RNN's are useful for sequence data. The promise of a recurrent neural network is to use the representation of a previous input to aid the representation of a later input.\n",
    "\n",
    "- MIT's sequence modelling lecture: https://www.youtube.com/watch?v=qjrad0V0uJE\n",
    "- Chris Olah's intro to LSTM: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- Andrej Karpathy's the unreasonable effectiveness of recurrent neural network: https://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "<div style=\"width: 720px; height:450px\">\n",
    "    <img src='https://miro.medium.com/v2/resize:fit:1400/1*3ltsv1uzGR6UBjZ6CUs04A.jpeg' style=\"width:100%\">\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model_2: LSTM\n",
    "\n",
    "LSTM - Long Short Term Memory (one of the most popular LSTM Cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an LSTM model\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.LSTM(units=64, return_sequences=True)(x)\n",
    "x = tf.keras.layers.LSTM(units=64)(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x) # binary classification model (0 or 1)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name='model_2_LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model_2_history = model_2.fit(x=train_sequences,\n",
    "                              y=train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sequences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(dir_name=tensorboard_logs, experiment_name='model_2_LSTM')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "model_2.evaluate(val_sequences, val_labels)\n",
    "\n",
    "# make predictions with LSTM Model 2\n",
    "model_2_pred_probs = model_2.predict(val_sequences)\n",
    "\n",
    "# convert model prediction probabilities to label format\n",
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_results = calculate_results(y_true=val_labels, y_pred=model_2_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = pd.DataFrame({'model_0_results': baseline_results,\n",
    "                              'model_1_results': model_1_results,\n",
    "                              'model_2_results': model_2_results})\n",
    "\n",
    "model_results.transpose().sort_values(by='f1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: GRU (Gated Recurrent Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.GRU(units=64, return_sequences=True)(x) # if you want to stack recurrent layer on top of each other, you need return sequences = True\n",
    "x = tf.keras.layers.GRU(units=64)(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "# x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x) # binary classification model (0 or 1)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name='model_3_GRU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_3_history = model_3.fit(x=train_sequences,\n",
    "                              y=train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sequences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(dir_name=tensorboard_logs, experiment_name='model_3_GRU')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "model_3.evaluate(val_sequences, val_labels)\n",
    "model_3_pred_probs = model_3.predict(val_sequences)\n",
    "\n",
    "# convert model prediction probabilities to label format\n",
    "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
    "\n",
    "model_3_results = calculate_results(y_true=val_labels, y_pred=model_3_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = pd.DataFrame({'model_0_results': baseline_results,\n",
    "                                  'model_1_results': model_1_results,\n",
    "                                  'model_2_results': model_2_results,\n",
    "                                  'model_3_results': model_3_results})\n",
    "model_results.transpose().sort_values(by='f1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=64, return_sequences=True))(x) # if you want to stack recurrent layer on top of each other, you need return sequences = True\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=64))(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x) # binary classification model (0 or 1)\n",
    "model_4 = tf.keras.Model(inputs, outputs, name='model_4_bidirectional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_4.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4_history = model_4.fit(x=train_sequences,\n",
    "                              y=train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sequences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(dir_name=tensorboard_logs, experiment_name='model_4_bidirectional')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "model_4.evaluate(val_sequences, val_labels)\n",
    "\n",
    "# make predictions\n",
    "model_4_pred_probs = model_4.predict(val_sequences)\n",
    "\n",
    "# convert model prediction probabilities to label format\n",
    "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
    "\n",
    "# calculate\n",
    "model_4_results = calculate_results(y_true=val_labels, y_pred=model_4_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = pd.DataFrame({'model_0_results': baseline_results,\n",
    "                                  'model_1_results': model_1_results,\n",
    "                                  'model_2_results': model_2_results,\n",
    "                                  'model_3_results': model_3_results,\n",
    "                                  'model_4_results': model_4_results})\n",
    "model_results.transpose().sort_values(by='f1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks for Text (and other type sequences)\n",
    "\n",
    "We've used CNNs for images but images are typically 2D (height vs width)... however, out text data is 1D.  \n",
    "Previously we've Conv2D for our image data but now we're going to use Conv1D.\n",
    "\n",
    "### Model 5: Conv1D\n",
    "\n",
    "see: https://poloclub.github.io/cnn-explainer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out our embedding layer, Conv1D layer and max pooling\n",
    "embedding_test = embedding(text_vectorizer(['this is a test sentences'])) # turn target\n",
    "conv_1d = tf.keras.layers.Conv1D(filters=32,\n",
    "                                 kernel_size=5, # this is also referred to as an ngram of 5 (meaning it looks at 5 words at a time)\n",
    "                                 strides=1, # default\n",
    "                                 activation='relu',\n",
    "                                 padding='valid',   # tensorshape is (1, 11, 32) with the use of the same would be (1, 15, 32)\n",
    "                                                    # default = \"valid\", the output is smaller than input shape, \"same\" means output is same shape as input\n",
    "                                 )\n",
    "\n",
    "conv_1d_output = conv_1d(embedding_test) # pass test embedding through conv1d layer\n",
    "max_pool = tf.keras.layers.GlobalMaxPool1D()\n",
    "max_pool_output = max_pool(conv_1d_output) # equivalent to get the most important feature or get the feature with the highest value\n",
    "\n",
    "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, activation='relu', padding='valid')(x)\n",
    "x = tf.keras.layers.GlobalMaxPool1D()(x)\n",
    "# x = tf.keras.layers.Dense(64, activation='relu')(x) \n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x) # binary classification model (0 or 1)\n",
    "model_5 = tf.keras.Model(inputs, outputs, name='model_5_conv1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_5.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5_history = model_5.fit(x=train_sequences,\n",
    "                              y=train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sequences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(dir_name=tensorboard_logs, experiment_name='model_5_conv1d')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "model_5.evaluate(val_sequences, val_labels)\n",
    "\n",
    "# make predictions\n",
    "model_5_pred_probs = model_5.predict(val_sequences)\n",
    "\n",
    "# convert model prediction probabilities to label format\n",
    "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
    "\n",
    "# calculate\n",
    "model_5_results = calculate_results(y_true=val_labels, y_pred=model_5_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = pd.DataFrame({'model_0_results': baseline_results,\n",
    "                                  'model_1_results': model_1_results,\n",
    "                                  'model_2_results': model_2_results,\n",
    "                                  'model_3_results': model_3_results,\n",
    "                                  'model_4_results': model_4_results,\n",
    "                                  'model_5_results': model_5_results})\n",
    "model_results.transpose().sort_values(by='f1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6: Tensorflow Hub Pretrained Sentence Encoder\n",
    "\n",
    "Now we've built a few of our own models, let's try and use transfer learning for NLP\n",
    "\n",
    "\n",
    "see: https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder?hl=pt-br  \n",
    "see: https://arxiv.org/abs/1803.11175  \n",
    "see: https://huggingface.co/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model using sequencial api\n",
    "model_6 = tf.keras.Sequential([\n",
    "    sentence_encoder_layer,\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "], name='model_6_USE')\n",
    "\n",
    "\n",
    "# compile the model\n",
    "model_6.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model_6_history = model_6.fit(x=train_sequences,\n",
    "                              y=train_labels,\n",
    "                              epochs=6,\n",
    "                              validation_data=(val_sequences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(dir_name=tensorboard_logs, \n",
    "                                                                     experiment_name='model_6_USE')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "model_6.evaluate(val_sequences, val_labels)\n",
    "\n",
    "# make predictions\n",
    "model_6_pred_probs = model_6.predict(val_sequences)\n",
    "\n",
    "# convert model prediction probabilities to label format\n",
    "model_6_preds = tf.squeeze(tf.round(model_6_pred_probs))\n",
    "\n",
    "# calculate\n",
    "model_6_results = calculate_results(y_true=val_labels, y_pred=model_6_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = pd.DataFrame({'model_0_results': baseline_results,\n",
    "                                  'model_1_results': model_1_results,\n",
    "                                  'model_2_results': model_2_results,\n",
    "                                  'model_3_results': model_3_results,\n",
    "                                  'model_4_results': model_4_results,\n",
    "                                  'model_5_results': model_5_results,\n",
    "                                  'model_6_results': model_6_results})\n",
    "model_results.transpose().sort_values(by='f1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 7: Tensorflow hub pretrained with 10% of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: Making data splits like below leads to data leakage (model_7 trained on 10% data)\n",
    "## DO NOT MAKE DATA SPLITS WHICH LEAK DATA FROM VALIDATION/TEST SET INTO TRAINING SET\n",
    "\n",
    "# create subset of 10% of the training data\n",
    "# train_10_percent = train_df_shuffled[['text', 'target']].sample(frac=0.1, random_state=42)\n",
    "# train_sentences_10_percent = train_10_percent['text'].to_list()\n",
    "# train_labels_10_percent = train_10_percent['target'].to_list()\n",
    "\n",
    "# len(train_sentences_10_percent), len(train_labels_10_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a better dataset split\n",
    "train_10_percent_split = int(0.1 * len(train_sequences))\n",
    "train_sentences_10_percent = train_sequences[:train_10_percent_split]\n",
    "train_labels_10_percent = train_labels[:train_10_percent_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model using sequencial api\n",
    "model_7 = tf.keras.Sequential([\n",
    "    sentence_encoder_layer,\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "], name='model_6_USE')\n",
    "\n",
    "\n",
    "# compile the model\n",
    "model_7.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model_7_history = model_7.fit(x=train_sequences,\n",
    "                              y=train_labels,\n",
    "                              epochs=6,\n",
    "                              validation_data=(val_sequences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(dir_name=tensorboard_logs, \n",
    "                                                                     experiment_name='model_7_USE')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "model_7.evaluate(val_sequences, val_labels)\n",
    "\n",
    "# make predictions\n",
    "model_7_pred_probs = model_7.predict(val_sequences)\n",
    "\n",
    "# convert model prediction probabilities to label format\n",
    "model_7_preds = tf.squeeze(tf.round(model_7_pred_probs))\n",
    "\n",
    "# calculate\n",
    "model_7_results = calculate_results(y_true=val_labels, y_pred=model_7_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = pd.DataFrame({'model_0_results': baseline_results,\n",
    "                                  'model_1_results': model_1_results,\n",
    "                                  'model_2_results': model_2_results,\n",
    "                                  'model_3_results': model_3_results,\n",
    "                                  'model_4_results': model_4_results,\n",
    "                                  'model_5_results': model_5_results,\n",
    "                                  'model_6_results': model_6_results,\n",
    "                                  'model_7_results': model_7_results})\n",
    "\n",
    "model_results = model_results.transpose().sort_values(by='accuracy', ascending=False)\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.plot(kind='bar', figsize=(20, 6)).legend(bbox_to_anchor=(1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading a trained Model\n",
    "\n",
    "There are two main formats to save a model in Tensorflow\n",
    "1. HDF5 formats\n",
    "2. The `savedModel` format (this is default when using tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6.save(f'{MODEL_PATH}/tfhub_nlp_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "# Load model with custom Hub Layer (required HDF5 format)\n",
    "# model_6_loaded = tf.keras.models.load_model(f'{MODEL_PATH}/tfhub_nlp_model.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "model_6_loaded = tf.keras.models.load_model(f'{MODEL_PATH}/08_model_6_USE_feature_extractor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 14s 25ms/step - loss: 0.4272 - accuracy: 0.8163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42723122239112854, 0.8162729740142822]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6_loaded.evaluate(val_sequences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TF Hub Sentence Encoder Model to SavedModel format (default)\n",
    "# model_6.save(f'{MODEL_PATH}/tfhub_nlp_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_6_saved_model = tf.keras.models.load_model(f'{MODEL_PATH}/tfhub_nlp_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the most wrong examples\n",
    "\n",
    "* if our best model still isn't perfect, what examples is it getting wrong?\n",
    "* and of these wrong examples which ones is it getting **most wrong** (those will prediction probabilities closet to the oposite class)\n",
    "\n",
    "for example if a sample should have a label 0 but our model predict a prediction probability of 0.999 (really close 1) and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a pretrained model from google storage\n",
    "from helper_functions import download_unzip_data\n",
    "\n",
    "filename = '08_model_6_USE_feature_extractor.zip'\n",
    "url = f'https://storage.googleapis.com/ztm_tf_course/{filename}'\n",
    "\n",
    "# download_unzip_data(url, filename, ZIP_PATH, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "# make predictions with the loaded model\n",
    "model_6_loaded_preds_prob = model_6_loaded.predict(val_sequences)\n",
    "model_6_loaded_preds = tf.squeeze(tf.round(model_6_loaded_preds_prob))\n",
    "\n",
    "model_6_loaded_results = calculate_results(y_true=val_labels, y_pred=model_6_loaded_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with validation sentences and best performing model predictions\n",
    "val_df = pd.DataFrame({'text': val_sequences,\n",
    "                       'target': val_labels,\n",
    "                       'prediction': model_6_loaded_preds,\n",
    "                       'prediction_probability': tf.squeeze(model_6_loaded_preds_prob)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prediction_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DFR EP016 Monthly Meltdown - On Dnbheaven 2015...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.159757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FedEx no longer to transport bioterror germs i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.747162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gunmen kill four in El Salvador bus attack: Su...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.988749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@camilacabello97 Internally and externally scr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.196229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Radiation emergency #preparedness starts with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.707808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  prediction  \\\n",
       "0  DFR EP016 Monthly Meltdown - On Dnbheaven 2015...       0         0.0   \n",
       "1  FedEx no longer to transport bioterror germs i...       0         1.0   \n",
       "2  Gunmen kill four in El Salvador bus attack: Su...       1         1.0   \n",
       "3  @camilacabello97 Internally and externally scr...       1         0.0   \n",
       "4  Radiation emergency #preparedness starts with ...       1         1.0   \n",
       "\n",
       "   prediction_probability  \n",
       "0                0.159757  \n",
       "1                0.747162  \n",
       "2                0.988749  \n",
       "3                0.196229  \n",
       "4                0.707808  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prediction_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>? High Skies - Burning Buildings ? http://t.co...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.910196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>FedEx will no longer transport bioterror patho...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.876982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>@noah_anyname That's where the concentration c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.852300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Ashes 2015: AustraliaÛªs collapse at Trent Br...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.835454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>@AshGhebranious civil rights continued in the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.827213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  target  prediction  \\\n",
       "31   ? High Skies - Burning Buildings ? http://t.co...       0         1.0   \n",
       "759  FedEx will no longer transport bioterror patho...       0         1.0   \n",
       "628  @noah_anyname That's where the concentration c...       0         1.0   \n",
       "209  Ashes 2015: AustraliaÛªs collapse at Trent Br...       0         1.0   \n",
       "251  @AshGhebranious civil rights continued in the ...       0         1.0   \n",
       "\n",
       "     prediction_probability  \n",
       "31                 0.910196  \n",
       "759                0.876982  \n",
       "628                0.852300  \n",
       "209                0.835454  \n",
       "251                0.827213  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the wrong predictions and sort by prediction probabilities\n",
    "most_wrong = val_df[val_df['target'] != val_df['prediction']].sort_values(by='prediction_probability', ascending=False)\n",
    "most_wrong.head(5) # these are false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prediction_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>@SoonerMagic_ I mean I'm a fan but I don't nee...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>I get to smoke my shit in peace</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Why are you deluged with low self-image? Take ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Reddit Will Now QuarantineÛ_ http://t.co/pkUA...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ron &amp;amp; Fez - Dave's High School Crush https...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  target  prediction  \\\n",
       "411  @SoonerMagic_ I mean I'm a fan but I don't nee...       1         0.0   \n",
       "233                    I get to smoke my shit in peace       1         0.0   \n",
       "38   Why are you deluged with low self-image? Take ...       1         0.0   \n",
       "244  Reddit Will Now QuarantineÛ_ http://t.co/pkUA...       1         0.0   \n",
       "23   Ron &amp; Fez - Dave's High School Crush https...       1         0.0   \n",
       "\n",
       "     prediction_probability  \n",
       "411                0.043918  \n",
       "233                0.042087  \n",
       "38                 0.038998  \n",
       "244                0.038949  \n",
       "23                 0.037186  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_wrong.tail(5) # these are false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  \n",
       "0                    Just happened a terrible car crash  \n",
       "1     Heard about #earthquake is different cities, s...  \n",
       "2     there is a forest fire at spot pond, geese are...  \n",
       "3              Apocalypse lighting. #Spokane #wildfires  \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
       "...                                                 ...  \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
       "3259  Storm in RI worse than last hurricane. My city...  \n",
       "3260  Green Line derailment in Chicago http://t.co/U...  \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
       "3262  #CityofCalgary has activated its Municipal Eme...  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The speed/score tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a function to measure the time of prediction\n",
    "import time\n",
    "\n",
    "def pred_timer(model, samples):\n",
    "    \"\"\"\n",
    "    Times how long a model takes to make predictions on samples\n",
    "    \"\"\"\n",
    "    # get start time\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # make predictions\n",
    "    model.predict(samples)\n",
    "    \n",
    "    # get finish time\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    # calculate how long prediction took to make\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    time_per_pred = total_time/len(samples)\n",
    "    return total_time, time_per_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5327792999996745, 0.0006991854330704391)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6_loaded_total_time, model_6_loaded_per_pred = pred_timer(model=model_6_loaded, \n",
    "                                                                samples=val_sequences)\n",
    "model_6_loaded_total_time, model_6_loaded_per_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.07200689999990573, 9.449724409436447e-05)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_loaded_total_time, model_0_loaded_per_pred = pred_timer(model=model_0, \n",
    "                                                                samples=val_sequences)\n",
    "model_0_loaded_total_time, model_0_loaded_per_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_0_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>79.265092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>78.621898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>81.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>79.265092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model_0_results\n",
       "accuracy         79.265092\n",
       "f1               78.621898\n",
       "precision        81.113900\n",
       "recall           79.265092"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.scatter(model_0_loaded_per_pred, baseline_results['f1'], label='baseline')\n",
    "plt.scatter(model_6_loaded_per_pred, model_6_loaded_results['f1'], label='tfhub_sequence_encoder')\n",
    "plt.legend()\n",
    "plt.title('F1-score versus time per prediction')\n",
    "plt.xlabel('Time per prediction')\n",
    "plt.ylabel('F1-score');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
